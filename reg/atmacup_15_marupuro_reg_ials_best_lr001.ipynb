{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb51b033-6fc4-4a1e-8f02-83a3a4e5d30f",
   "metadata": {},
   "source": [
    "# #15 atmaCup 正則化の強さを変えたときの精度 (iALS) 提出用\n",
    "\n",
    "比較対象：\n",
    "\n",
    "- SVD (正則化なし)\n",
    "- SVD (正則化あり)  ← 本notebookはこれを、最も良かったハイパーパラメーターを使って学習率0.01で学習させなおしたもの\n",
    "- NMF (正則化なし)\n",
    "- NMF (正則化あり)\n",
    "- 行列分解なし"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f62238-84f9-45f0-9f3a-8d6eacf3317b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23589dbc-0e24-418f-8af9-38151531a96e",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de263aa-5de5-4ca8-a475-c1fb48ee7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565757ad-6db9-41cc-9c67-204eac6f2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "anime = pd.read_csv('data/anime.csv', na_values=['Unknown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1b7c9-525a-4a81-9388-827561e3ef24",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a18452-8203-4d68-81bf-aca786535ac2",
   "metadata": {},
   "source": [
    "### 共通の前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cedd934-6d31-485c-bbe4-d5fed8868de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anime = train.merge(anime, how='left', on='anime_id')\n",
    "test_anime = test.merge(anime, how='left', on='anime_id')\n",
    "traintest_anime = pd.concat([train_anime, test_anime], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b44601-0836-4280-8f5d-2ab971446b29",
   "metadata": {},
   "source": [
    "### ユーザーとアニメの分散表現の獲得"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7003a-d05a-469e-b9b9-9a4f0973cce3",
   "metadata": {},
   "source": [
    "ここでは、iALSのハイパーパラメーター調整の重要性を説いた以下の論文の[再現コード](https://github.com/google-research/google-research/blob/master/ials/ncf_benchmarks/ials.py)からiASLの実装を抜粋して用いる。  \n",
    "[Rendle, Steffen, et al. \"Revisiting the performance of ials on item recommendation benchmarks.\" RecSys 2022.](https://dl.acm.org/doi/10.1145/3523227.3548486)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155f085-e194-47e1-9c1c-aea03c246ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "class IALSDataSet():\n",
    "    def __init__(self, train_by_user, train_by_item, num_batches):\n",
    "        self.train_by_user = train_by_user\n",
    "        self.train_by_item = train_by_item\n",
    "        self.num_users = len(train_by_user)\n",
    "        self.num_items = len(train_by_item)\n",
    "        self.user_batches = self._batch(train_by_user, num_batches)\n",
    "        self.item_batches = self._batch(train_by_item, num_batches)\n",
    "\n",
    "    def _batch(self, xs, num_batches):\n",
    "        batches = [[] for _ in range(num_batches)]\n",
    "        for i, x in enumerate(xs):\n",
    "            batches[i % num_batches].append(x)\n",
    "        return batches\n",
    "\n",
    "class IALS():\n",
    "    def __init__(self, num_users, num_items, embedding_dim, reg, unobserved_weight, stddev):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.reg = reg\n",
    "        self.unobserved_weight = unobserved_weight\n",
    "        self.user_embedding = np.random.normal(0, stddev, (num_users, embedding_dim))\n",
    "        self.item_embedding = np.random.normal(0, stddev, (num_items, embedding_dim))\n",
    "        self._update_user_gramian()\n",
    "        self._update_item_gramian()\n",
    "\n",
    "    def _update_user_gramian(self):\n",
    "        self.user_gramian = np.matmul(self.user_embedding.T, self.user_embedding)\n",
    "\n",
    "    def _update_item_gramian(self):\n",
    "        self.item_gramian = np.matmul(self.item_embedding.T, self.item_embedding)\n",
    "\n",
    "    def score(self, user_history):\n",
    "        user_emb = project(user_history, self.item_embedding, self.item_gramian, self.reg, self.unobserved_weight)\n",
    "        result = np.dot(user_emb, self.item_embedding.T)\n",
    "        return result\n",
    "\n",
    "    def train(self, ds):\n",
    "        self._solve(ds.user_batches, is_user=True)\n",
    "        self._update_user_gramian()\n",
    "        self._solve(ds.item_batches, is_user=False)\n",
    "        self._update_item_gramian()\n",
    "\n",
    "    def _solve(self, batches, is_user):\n",
    "        if is_user:\n",
    "            embedding = self.user_embedding\n",
    "            args = (self.item_embedding, self.item_gramian, self.reg, self.unobserved_weight)\n",
    "        else:\n",
    "            embedding = self.item_embedding\n",
    "            args = (self.user_embedding, self.user_gramian, self.reg, self.unobserved_weight)\n",
    "        results = map_parallel(solve, batches, *args)\n",
    "        for r in results:\n",
    "            for user, emb in r.items():\n",
    "                embedding[user, :] = emb\n",
    "\n",
    "def map_parallel(fn, xs, *args):\n",
    "    if len(xs) == 1:\n",
    "        return [fn(xs[0], *args)]\n",
    "    num_threads = len(xs)\n",
    "    executor = concurrent.futures.ProcessPoolExecutor(num_threads)\n",
    "    futures = [executor.submit(fn, x, *args) for x in xs]\n",
    "    concurrent.futures.wait(futures)\n",
    "    results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "def solve(data_by_user, item_embedding, item_gramian, global_reg, unobserved_weight):\n",
    "    user_embedding = {}\n",
    "    for user, items in data_by_user:\n",
    "        reg = global_reg *(len(items) + unobserved_weight * item_embedding.shape[0])\n",
    "        user_embedding[user] = project(items, item_embedding, item_gramian, reg, unobserved_weight)\n",
    "    return user_embedding\n",
    "\n",
    "def project(user_history, item_embedding, item_gramian, reg, unobserved_weight):\n",
    "    if not user_history:\n",
    "        raise ValueError(\"empty user history in projection\")\n",
    "    emb_dim = np.shape(item_embedding)[1]\n",
    "    lhs = np.zeros([emb_dim, emb_dim])\n",
    "    rhs = np.zeros([emb_dim])\n",
    "    for item in user_history:\n",
    "        item_emb = item_embedding[item]\n",
    "        lhs += np.outer(item_emb, item_emb)\n",
    "        rhs += item_emb\n",
    "    lhs += unobserved_weight * item_gramian\n",
    "    lhs = lhs + np.identity(emb_dim) * reg\n",
    "    return np.linalg.solve(lhs, rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ff466-0478-4ecb-8fe0-e5c814ab696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "class UserAnimeIALSTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, traintest_anime, embedding_dim, reg, unobserved_weight, stddev, num_epochs, num_batches):\n",
    "        self.traintest_anime = traintest_anime\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.reg = reg\n",
    "        self.unobserved_weight = unobserved_weight\n",
    "        self.stddev = stddev\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_batches = num_batches\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        le_user = LabelEncoder().fit(self.traintest_anime['user_id'])\n",
    "        le_item = LabelEncoder().fit(self.traintest_anime['anime_id'])\n",
    "\n",
    "        train_by_user = (\n",
    "            self.traintest_anime\n",
    "            .assign(user_id=lambda df: le_user.transform(df['user_id']))\n",
    "            .assign(anime_id=lambda df: le_item.transform(df['anime_id']))\n",
    "            .groupby('user_id')['anime_id'].apply(list)\n",
    "            .pipe(lambda s: list(zip(s.index, s)))\n",
    "        )\n",
    "        train_by_item = (\n",
    "            self.traintest_anime\n",
    "            .assign(user_id=lambda df: le_user.transform(df['user_id']))\n",
    "            .assign(anime_id=lambda df: le_item.transform(df['anime_id']))\n",
    "            .groupby('anime_id')['user_id'].apply(list)\n",
    "            .pipe(lambda s: list(zip(s.index, s)))\n",
    "        )\n",
    "\n",
    "        train_ds = IALSDataSet(train_by_user, train_by_item, self.num_batches)\n",
    "        self.ials = IALS(\n",
    "            num_users=train_ds.num_users,\n",
    "            num_items=train_ds.num_items,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            reg=self.reg,\n",
    "            unobserved_weight=self.unobserved_weight,\n",
    "            stddev=self.stddev\n",
    "        )\n",
    "\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            self.ials.train(train_ds)\n",
    "        self.user_embedding = pd.DataFrame(\n",
    "            data=self.ials.user_embedding,\n",
    "            index=pd.Index(le_user.classes_, name='user_id'),\n",
    "            columns=[f'user_svd_{i:04d}' for i in range(self.embedding_dim)]\n",
    "        )\n",
    "        self.item_embedding = pd.DataFrame(\n",
    "            data=self.ials.item_embedding,\n",
    "            index=pd.Index(le_item.classes_, name='anime_id'),\n",
    "            columns=[f'item_svd_{i:04d}' for i in range(self.embedding_dim)]\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        target_columns = ['user_id', 'anime_id']\n",
    "        X_new = (\n",
    "            X[target_columns]\n",
    "            .merge(self.user_embedding, how='left', on='user_id')\n",
    "            .merge(self.item_embedding, how='left', on='anime_id')\n",
    "            .drop(columns=target_columns)\n",
    "            .to_numpy()\n",
    "        )\n",
    "\n",
    "        return X_new\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        names = np.concatenate([\n",
    "            self.user_embedding.columns.to_numpy(),\n",
    "            self.item_embedding.columns.to_numpy()\n",
    "        ])\n",
    "\n",
    "        return names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303545b3-0602-4877-a503-4e96ffa39cfa",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d98597-52b3-4f13-b808-012b739d8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "# テストデータと同じように学習データに存在しないユーザーの評価結果が約23％を占めるようデータを分割する。\n",
    "class UnknownUserKFold:\n",
    "    def __init__(self, n_splits_cv, n_splits_uu):\n",
    "        self.n_splits_cv = n_splits_cv\n",
    "        self.n_splits_uu = n_splits_uu\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        splits_cv = KFold(n_splits=self.n_splits_cv, shuffle=True, random_state=0).split(X)\n",
    "        splits_uu = GroupKFold(n_splits=self.n_splits_uu).split(X, groups=groups)\n",
    "        for fold in range(self.n_splits_cv):\n",
    "            train_index, test_index = next(splits_cv)\n",
    "            _, uu_index = next(splits_uu)\n",
    "            train_index = np.setdiff1d(train_index, uu_index)\n",
    "            test_index = np.union1d(test_index, uu_index)\n",
    "\n",
    "            yield train_index, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ec20a-5c8c-49f7-9306-11b53c74b9b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "reg = 0.01\n",
    "unobserved_weight = 0.3\n",
    "\n",
    "transformer_path = pathlib.Path(f'temp/transformer_emb{embedding_dim:03d}_reg{reg:.4f}_uw{unobserved_weight:.3f}_ep16.pkl')\n",
    "if transformer_path.exists():\n",
    "    with open(transformer_path, 'rb') as f:\n",
    "        transformer = pickle.load(f)\n",
    "else:\n",
    "    transformer = UserAnimeIALSTransformer(\n",
    "        traintest_anime,\n",
    "        embedding_dim=embedding_dim,\n",
    "        reg=reg,\n",
    "        unobserved_weight=unobserved_weight,\n",
    "        stddev = 0.1 / np.sqrt(embedding_dim),\n",
    "        num_epochs=16,  # 8 -> 16\n",
    "        num_batches=1\n",
    "    )\n",
    "    transformer = transformer.set_output(transform='pandas')\n",
    "    transformer = transformer.fit(train_anime)\n",
    "    with open(transformer_path, 'wb') as f:\n",
    "        pickle.dump(transformer, f)\n",
    "\n",
    "x_train = transformer.transform(train_anime)\n",
    "y_train = train['score']\n",
    "user_id_train = train['user_id']\n",
    "\n",
    "models = []\n",
    "scores = []\n",
    "kf = UnknownUserKFold(n_splits_cv=5, n_splits_uu=18)\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(x_train, groups=user_id_train)):\n",
    "    cv_x_train = x_train.iloc[train_index, :]\n",
    "    cv_y_train = y_train.iloc[train_index]\n",
    "    cv_x_test = x_train.iloc[test_index, :]\n",
    "    cv_y_test = y_train.iloc[test_index]\n",
    "\n",
    "    model = lgb.train(\n",
    "        params={\n",
    "            'objective': 'regression',\n",
    "            'verbose': -1,\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.01,  # 0.05 -> 0.01\n",
    "            'num_leaves': 100,\n",
    "            'feature_fraction': 0.7,\n",
    "            'seed': 127\n",
    "        },\n",
    "        train_set=lgb.Dataset(cv_x_train, label=cv_y_train),\n",
    "        valid_sets=[lgb.Dataset(cv_x_test, label=cv_y_test)],\n",
    "        num_boost_round=100000,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=500, verbose=True)]  # 100 -> 500\n",
    "    )\n",
    "    models.append(model)\n",
    "    scores.append(model.best_score['valid_0']['rmse'])\n",
    "    print('')\n",
    "\n",
    "print(f'cv: {np.mean(scores):.4f} ± {np.std(scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382df43d-eb06-475e-a05b-c6c4d934b3f2",
   "metadata": {},
   "source": [
    "## 予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c0599-d72b-43fc-80e1-e59a6ecf6a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = transformer.transform(test_anime)\n",
    "y_test_pred = np.mean([model.predict(x_test) for model in models], axis=0)\n",
    "\n",
    "sub = pd.DataFrame(data={'score': y_test_pred})\n",
    "sub.loc[sub['score'] < 1, 'score'] = 1\n",
    "sub.loc[sub['score'] > 10, 'score'] = 10\n",
    "sub.to_csv(f'submission/submission_reg_ials_best_lr001.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
